{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global libs \n",
    "import logging \n",
    "from datetime import datetime \n",
    "from collections import defaultdict\n",
    "import math\n",
    "import os\n",
    "\n",
    "# ML/DS libs\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, GradientBoostingClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import f1_score, log_loss, precision_score, recall_score, roc_auc_score, accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "# Local libs \n",
    "import reader\n",
    "import preprocessing\n",
    "import features\n",
    "import training \n",
    "import selection\n",
    "import visualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions = [\n",
    "    'PRE',\n",
    "    'POST',\n",
    "    'TEST01',\n",
    "    'TEST02',\n",
    "    'TEST03',\n",
    "    'TEST04',\n",
    "    'TEST05',\n",
    "    'TEST06',\n",
    "    'TEST07',\n",
    "    'TEST08'\n",
    "]\n",
    "\n",
    "participants = range(1, 10, 1) # P001 - P073\n",
    "\n",
    "raw_dir = 'data/raw_data/'\n",
    "extracted_dir = 'data/extracted_data_old/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_feature = 'annotation'\n",
    "target_key = 'R1'\n",
    "target_keys = [target_key]\n",
    "target_threshold = 3\n",
    "\n",
    "target_function = lambda df : features.format_annotation(\n",
    "    df,\n",
    "    window_size=20.0,\n",
    "    stride=5.0,\n",
    "    window_fn=lambda x : np.mean(x, axis=0),\n",
    "    threshold=target_threshold,\n",
    "    time_key='Time (s)',\n",
    "    target_keys=target_keys\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM feature, TO feature, Extraction Function, Format Function, Whether or not to write extraction, Use existing\n",
    "features_to_extract = [\n",
    "\n",
    "    [\n",
    "        'audio',\n",
    "        'wav',\n",
    "        'eGeMAPSv02_20sec_5sec',\n",
    "        lambda df : features.get_audio_features(df['Audio'].to_numpy(), df.sr, 20.0, 5.0, 'eGeMAPSv02', 'LLDs'),\n",
    "        lambda df : features.format_extracted_features(\n",
    "            df,\n",
    "            time_key='Time (s)',\n",
    "            shift_fn=lambda df : preprocessing.shift_dataframe(df, 4, False)\n",
    "        ),\n",
    "        False,\n",
    "        False\n",
    "    ],\n",
    "    [\n",
    "        'audio',\n",
    "        'wav',\n",
    "        'eGeMAPSv02_10sec_5sec',\n",
    "        lambda df : features.get_audio_features(df['Audio'].to_numpy(), df.sr, 10.0, 5.0, 'eGeMAPSv02', 'LLDs'),\n",
    "        lambda df : features.format_extracted_features(\n",
    "            df,\n",
    "            time_key='Time (s)',\n",
    "            sampling_fn=lambda df : preprocessing.downsample_dataframe(df, 2, 'last'),\n",
    "            shift_fn=lambda df : preprocessing.shift_dataframe(df, 4, False)\n",
    "        ),\n",
    "        False,\n",
    "        False\n",
    "    ],\n",
    "    [\n",
    "        'E4_EDA_PPT', \n",
    "        'excel',\n",
    "        'EDA_20sec_5sec',\n",
    "        lambda df : features.get_EDA_features(df['EDA'].to_numpy(), 4, 20.0, 5.0, df['Time (s)'].to_numpy()),\n",
    "        lambda df : features.format_extracted_features(\n",
    "            df,\n",
    "            time_key='Time (s)',\n",
    "            shift_fn=lambda df : preprocessing.shift_dataframe(df, 4, False)\n",
    "        ),\n",
    "        False,\n",
    "        False\n",
    "    ],\n",
    "\n",
    "    [\n",
    "        'E4_BVP_PPT', \n",
    "        'excel',\n",
    "        'HRV_20sec_5sec',\n",
    "        lambda df : features.get_HRV_features(df['BVP'].to_numpy(), 64, 20.0, 5.0, df['Time (s)'].to_numpy()),\n",
    "        lambda df : features.format_extracted_features(\n",
    "            df,\n",
    "            time_key='Time (s)',\n",
    "            shift_fn=lambda df : preprocessing.shift_dataframe(df, 4, False)\n",
    "        ),\n",
    "        False,\n",
    "        False\n",
    "    ]\n",
    "]\n",
    "\n",
    "data_features = list(set([f[0] for f in features_to_extract]))\n",
    "data_formats = list(set([(f[0], f[1]) for f in features_to_extract])) # To remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_data, features_missing = reader.get_pts_data(raw_dir, data_formats, participants, sessions)\n",
    "\n",
    "target_data, target_missing = reader.get_pts_data(extracted_dir, [(target_feature, 'excel')], participants, sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_search = data_features.copy()\n",
    "feature_search.append(target_feature)\n",
    "valid_pts_sessions = training.get_valid_pts_sessions(\n",
    "    participants, \n",
    "    [features_missing, target_missing],\n",
    "    sessions,\n",
    "    feature_search\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid sessions for Participant 1: ['PRE', 'TEST01', 'TEST02', 'TEST03', 'TEST04']\n",
      "Valid sessions for Participant 3: ['PRE']\n",
      "Valid sessions for Participant 4: ['PRE', 'TEST01', 'TEST02', 'TEST03', 'TEST04', 'TEST05', 'TEST06', 'TEST07', 'TEST08']\n",
      "Valid sessions for Participant 5: ['PRE', 'POST', 'TEST01', 'TEST02', 'TEST03', 'TEST04', 'TEST05', 'TEST06', 'TEST07', 'TEST08']\n",
      "Valid sessions for Participant 6: ['PRE']\n",
      "Valid sessions for Participant 8: ['PRE', 'POST', 'TEST01', 'TEST02', 'TEST03', 'TEST04', 'TEST05', 'TEST06', 'TEST07', 'TEST08']\n",
      "Valid sessions for Participant 9: ['PRE']\n"
     ]
    }
   ],
   "source": [
    "pt_dfs = training.get_pt_dfs(features_data, target_data, valid_pts_sessions, target_feature, target_function, features_to_extract, extracted_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['t0' 'tn'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-16fb942233b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpt_dfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mpt_dfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m't0'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'tn'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4168\u001b[0m             \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4169\u001b[0m             \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4170\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4171\u001b[0m         )\n\u001b[1;32m   4172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mdrop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   3885\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3886\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3887\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_drop_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3889\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_drop_axis\u001b[0;34m(self, labels, axis, level, errors)\u001b[0m\n\u001b[1;32m   3938\u001b[0m                 \u001b[0mlabels_missing\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0merrors\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"raise\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlabels_missing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3940\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{labels} not found in axis\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3942\u001b[0m             \u001b[0mslicer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['t0' 'tn'] not found in axis\""
     ]
    }
   ],
   "source": [
    "for key in pt_dfs.keys():\n",
    "    pt_dfs[key].fillna(0, inplace=True)\n",
    "    #pt_dfs[key].drop(columns=['t0','tn'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loudness_sma3 alphaRatio_sma3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[       nan,        nan],\n",
       "       [       nan,        nan],\n",
       "       [       nan, 6.2723855 ],\n",
       "       ...,\n",
       "       [6.27255284, 6.27780145],\n",
       "       [6.27373301, 6.27360075],\n",
       "       [6.26931072, 6.27454259]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "array([[        nan,         nan],\n",
       "       [        nan,         nan],\n",
       "       [        nan, -5.0068742 ],\n",
       "       ...,\n",
       "       [-4.95775782, -4.94313005],\n",
       "       [-4.91847036, -4.96212897],\n",
       "       [-4.9237738 , -4.9046453 ]])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "index 148 is out of bounds for axis 0 with size 4",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-0b6956e1d7e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Drop any keys that are too correlated with eachother (keeps one)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_dropped_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorr_kept_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_within_correlations\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Drop any keys with stddev of 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Programming/TAMU/Research/VerBIO/verbio-repo/verbio/selection.py\u001b[0m in \u001b[0;36mdrop_within_correlations\u001b[0;34m(df, lower_bound, upper_bound, feature_keys)\u001b[0m\n\u001b[1;32m    129\u001b[0m                                 \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m                                 \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m                                 \u001b[0mcorrelation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspearmanr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_feature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcorrelation\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mupper_bound\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mcorrelation\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mlower_bound\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropped_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mkept_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/scipy/stats/stats.py\u001b[0m in \u001b[0;36mspearmanr\u001b[0;34m(a, b, axis, nan_policy)\u001b[0m\n\u001b[1;32m   4231\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSpearmanrResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4232\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4233\u001b[0;31m         \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvariable_has_nan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4234\u001b[0m         \u001b[0mrs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariable_has_nan\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mSpearmanrResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 148 is out of bounds for axis 0 with size 4"
     ]
    }
   ],
   "source": [
    "# Combine all \n",
    "df_all = pd.concat(pt_dfs.values(), axis=0, ignore_index=True)\n",
    "\n",
    "# Holds normalized dataframes\n",
    "normalized_pt_dfs = {}\n",
    "\n",
    "# All keys except target key\n",
    "feature_keys = [key for key in df_all.columns if key != target_key]\n",
    "\n",
    "# Compute mean and std over all feature keys\n",
    "df_mean = df_all[feature_keys].mean(axis=0)\n",
    "df_std = df_all[feature_keys].std(axis=0)\n",
    "\n",
    "# Drop any keys that are too correlated with eachother (keeps one)\n",
    "_, corr_dropped_keys, corr_kept_keys = selection.drop_within_correlations(df_all, -0.9, 0.9, feature_keys)\n",
    "\n",
    "# Drop any keys with stddev of 0\n",
    "invalid_features = corr_dropped_keys\n",
    "for feature in feature_keys:\n",
    "    if math.isclose(df_std[feature], 0.0): # Remove 0 std features\n",
    "        invalid_features.append(feature)\n",
    "\n",
    "# Keep list of features we kept\n",
    "valid_features = [f for f in feature_keys if f not in invalid_features]\n",
    "        \n",
    "# Drop all the invalid features from our mean and std\n",
    "df_mean.drop(labels=invalid_features, inplace=True)\n",
    "df_std.drop(labels=invalid_features, inplace=True)\n",
    "    \n",
    "# Normalize the keys we're keeping to 0 mean 1 std\n",
    "for pt in pt_dfs.keys():\n",
    "    normalized_pt_dfs[pt] = pt_dfs[pt].drop(invalid_features, axis=1)\n",
    "    normalized_pt_dfs[pt][valid_features] = normalized_pt_dfs[pt][valid_features].sub(df_mean, axis=1).div(df_std, axis=1)\n",
    "    \n",
    "# Do the same for the whole dataframe\n",
    "df_all.drop(invalid_features, axis=1, inplace=True)\n",
    "df_all[valid_features] = df_all[valid_features].sub(df_mean, axis=1).div(df_std, axis=1)\n",
    "\n",
    "# Resample the combined dataframe to equalize distribution\n",
    "df_all_resampled = training.eq_class_dist(df_all, target_key, [0,1], 'under')\n",
    "\n",
    "# Apply LASSO to combined dataframe\n",
    "dropped_keys, kept_keys = selection.select_by_LASSO(df_all_resampled, target_key)\n",
    "print('Dropped:', dropped_keys)\n",
    "print('\\nKept:', kept_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kept_keys' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-ee32087d4e3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Only keep the keys that made it through lasso and other filtering methods\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mfinal_dfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkept_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfinal_keys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kept_keys' is not defined"
     ]
    }
   ],
   "source": [
    "# Only keep the keys that made it through lasso and other filtering methods\n",
    "final_dfs = {}\n",
    "final_keys = kept_keys.copy()\n",
    "final_keys.append(target_key)\n",
    "\n",
    "for pt, df in pt_dfs.items():\n",
    "    final_dfs[pt] = df[final_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Equalize class distribution again for easier viewing\n",
    "df_plot = training.eq_class_dist(df_all, target_key, [0, 1], method='under')\n",
    "print('Plotting final keys')\n",
    "plt.clf()\n",
    "plt.figure(figsize=(32, 32))\n",
    "sns_plot = sns.pairplot(df_plot[final_keys], hue=target_key)\n",
    "sns_plot.savefig(\"plots/plot.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=100, random_state=500)\n",
    "#model = LinearSVC(C=0.1, penalty='l2', dual=False, max_iter=10000)\n",
    "#model = SVC(C=0.01, random_state=500, max_iter=100000) # Need to have max_iter otherwise this can take forever\n",
    "#model = LogisticRegression(max_iter=10000, C=0.1)\n",
    "#model = GradientBoostingClassifier(n_estimators=300, max_depth=2, learning_rate=0.1)\n",
    "\n",
    "#metrics = training.LOOCV_subject(final_dfs.keys(), final_dfs, 'R1', [0, 1], model, resample_method='under', show_confusion=True)\n",
    "metrics = training.k_fold_CV(final_dfs.keys(), final_dfs, 'R1', [0, 1], model, n_folds=10, resample_method='under', show_confusion=True)\n",
    "\n",
    "for key in ['accuracy','recall','precision','f1']:\n",
    "    print(f'Average {key} = {np.mean(metrics[key])}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
